{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauratomokiyo/imspeaking/blob/main/EdTechProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg4ER_-EianX"
      },
      "source": [
        "# Ed Tech Project - Laura Tomokiyo\n",
        "## Instructions\n",
        "\n",
        "To run this notebook, you will run each code block independently.  At the top left of each block is an arrow.  Click on that arrow and wait until you get a check mark beside it.  Some of them take some time to run, and you will see some output messages.  If you get something like \"error\" at the end, or get a red mark instead of a green check mark, it means that there is a problem with the code.  Please let me know so I can fix it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVLzQ1oEjOdo"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcjQHmCot7Io",
        "outputId": "f0186cc0-5293-49e8-9552-062f7f37e6b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: ipywebrtc in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (6.5.5)\n",
            "Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchaudio) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchaudio) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from notebook) (6.3.3)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook) (23.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from notebook) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook) (5.7.2)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook) (6.1.12)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook) (1.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from notebook) (5.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8,>=5.3.4->notebook) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (3.0.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (2.18.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook) (21.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->notebook) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (3.0.48)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook) (1.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client<8,>=5.3.4->notebook) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->notebook) (0.8.4)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook) (1.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook) (0.2.13)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook) (1.2.2)\n",
            "Requirement already satisfied: ipywebrtc in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "Requirement already satisfied: praatio in /usr/local/lib/python3.10/dist-packages (6.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from praatio) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "# Initial imports and installs\n",
        "!sudo apt install ffmpeg\n",
        "!pip install torchaudio ipywebrtc notebook\n",
        "!pip install ipywebrtc --upgrade\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "!pip install praatio\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "import scipy\n",
        "from IPython.display import Audio, display,HTML\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io.wavfile\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "from praatio import textgrid\n",
        "\n",
        "mode = 'offline' # offline or live"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzkV_FxW_zE4"
      },
      "source": [
        "##MFA Setup\n",
        "\n",
        "This section sets up the Montreal Forced Aligner.  It takes some time to load and run.  The forced aligner is used to find the boundaries between words and phonemes in the speech signal.\n",
        "\n",
        "Credit https://gist.github.com/NTT123/12264d15afad861cb897f7a20a01762e\n",
        "\n",
        "Credit https://eleanorchodroff.com/tutorial/montreal-forced-aligner.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf8ItDBr7cd4",
        "outputId": "58c04b75-b6ba-4be6-d80b-5856b8b1cbbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting install_mfa.sh\n"
          ]
        }
      ],
      "source": [
        "# MFA setup\n",
        "\n",
        "%%writefile install_mfa.sh\n",
        "#!/bin/bash\n",
        "\n",
        "## a script to install Montreal Forced Aligner (MFA)\n",
        "\n",
        "root_dir=${1:-/tmp/mfa}\n",
        "mkdir -p $root_dir\n",
        "cd $root_dir\n",
        "\n",
        "# download miniconda3\n",
        "wget -q --show-progress https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "bash Miniconda3-latest-Linux-x86_64.sh -b -p $root_dir/miniconda3 -f\n",
        "\n",
        "# create py38 env\n",
        "#$root_dir/miniconda3/bin/conda create -n aligner -c conda-forge openblas python=3.8 openfst pynini ngram baumwelch -y\n",
        "$root_dir/miniconda3/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n",
        "#$root_dir/miniconda3/bin/conda create -n aligner -c conda-forge python=3.11 montreal-forced-aligner=3.0.2 kalpy=0.6.2 kaldi=5.5.1112=cpu*\n",
        "source $root_dir/miniconda3/bin/activate aligner\n",
        "\n",
        "# install mfa, download kaldi\n",
        "pip install montreal-forced-aligner\n",
        "pip install git+https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner.git # install latest updates\n",
        "conda install -yq pytorch torchvision torchaudio cpuonly -c pytorch\n",
        "\n",
        "#mfa thirdparty download\n",
        "mfa download\n",
        "\n",
        "echo -e \"\\n======== DONE ==========\"\n",
        "echo -e \"\\nTo activate MFA, run: source $root_dir/miniconda3/bin/activate aligner\"\n",
        "echo -e \"\\nTo delete MFA, run: rm -rf $root_dir\"\n",
        "echo -e \"\\nSee: https://montreal-forced-aligner.readthedocs.io/en/latest/aligning.html to know how to use MFA\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y_5kBeeDlbi"
      },
      "source": [
        "Important: the next code block requires the user to respond \"y\" to a \"y/n\" question during the installation process.  Watch the output of the installation until you see this question, and then click at the end of that line of text and type \"y\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mut2ulWZ_HrE",
        "outputId": "9c443fcf-853c-497f-b50e-14f871543495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Miniconda3-latest-L 100%[===================>] 141.46M   141MB/s    in 1.0s    \n",
            "PREFIX=/tmp/mfa/miniconda3\n",
            "Unpacking payload ...\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /tmp/mfa/miniconda3\n",
            "WARNING: A conda environment already exists at '/tmp/mfa/miniconda3/envs/aligner'\n",
            "\n",
            "Remove existing environment?\n",
            "This will remove ALL directories contained within this specified prefix directory, including any other conda environments.\n",
            "\n",
            " (y/[n])? "
          ]
        }
      ],
      "source": [
        "# download and install mfa\n",
        "INSTALL_DIR=\"/tmp/mfa\" # path to install directory\n",
        "\n",
        "# make sure the environment and dependencies are right for MFA\n",
        "!bash ./install_mfa.sh {INSTALL_DIR}\n",
        "!source {INSTALL_DIR}/miniconda3/bin/activate aligner; mfa align --help\n",
        "\n",
        "# install sox tool\n",
        "!sudo apt install -q -y sox\n",
        "\n",
        "# download an english acoustic model (needs to be downloaded because we're in a colab?)\n",
        "!wget -q --show-progress https://github.com/MontrealCorpusTools/mfa-models/raw/main/acoustic/english.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvB1RA5os2gn"
      },
      "outputs": [],
      "source": [
        "# Get audio files\n",
        "# these are the reference files that have been pre-recorded, and also pre-recorded learner files for debugging\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lauratomokiyo/imspeaking/main/r4.zip\n",
        "!wget https://raw.githubusercontent.com/lauratomokiyo/imspeaking/main/rm4.zip\n",
        "!wget https://raw.githubusercontent.com/lauratomokiyo/imspeaking/main/promptlex.txt\n",
        "!unzip /content/r4.zip  # learner\n",
        "!unzip /content/rm.zip\n",
        "!unzip /content/rm4.zip # male reference\n",
        "\n",
        "reffiledir = \"/content/reference_audio/\"\n",
        "\n",
        "learnerfiledir = \"/content/learner_baseline_audio/\"\n",
        "if mode == 'offline':\n",
        "  !wget https://raw.githubusercontent.com/lauratomokiyo/imspeaking/main/f.zip\n",
        "  !unzip /content/f.zip\n",
        "\n",
        "tempalignmentdir = '/content/mfa/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJzW71vATBwl"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "  \"I'd like to make an appointment.\",\n",
        "  \"Can I change my flight return date?\",\n",
        "  \"I'd like the blue cheese burger with bacon.\",\n",
        "  \"How can I get to the train station?\",\n",
        "  \"That's a difficult situation.\",\n",
        "  \"Could we get the check, please?\",\n",
        "  \"Does this bus go to the airport?\",\n",
        "  \"You could ask him.\",\n",
        "  \"Is it okay if I join you?\",\n",
        "  \"Tariffs are paid by the consumer.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoZM3xW2S3yJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhqI44V8BPsW"
      },
      "outputs": [],
      "source": [
        "reffiledir = \"/content/reference_audio_male/\"\n",
        "\n",
        "def get_duration_statistics(tier):\n",
        "  vowel_count = 0\n",
        "  vowel_duration = 0\n",
        "  for (start,stop,word) in tier.entries:\n",
        "    if any(digit in word for digit in ['0','1','2']):\n",
        "      vowel_count += 1\n",
        "      vowel_duration += (stop-start)\n",
        "\n",
        "  nwords = len(tier)\n",
        "  utt_start = tier.entries[0][0]\n",
        "  utt_end = tier.entries[-1][1]\n",
        "  speaking_duration = utt_end-utt_start\n",
        "  speaking_rate = nwords/speaking_duration\n",
        "  ave_vowel_dur = vowel_duration / vowel_count\n",
        "#  print(f'there is {speaking_duration} total speaking time and {nwords} phonemes')\n",
        "  return(speaking_rate,ave_vowel_dur)\n",
        "\n",
        "def plot_contours(idx,mode='live',recorder_object=None):\n",
        "#idx = 1\n",
        "  ref_file = f'{reffiledir}r{idx}.wav'\n",
        "  learner_file = f'f{idx}.wav'\n",
        "  print(f'learner file is {learner_file}')\n",
        "\n",
        "  # write file if needed and load audio\n",
        "  if mode == 'live':\n",
        "#    recorder_object = f'recorder{idx}.audio.value'\n",
        "#    print(f'recorder object is {recorder_object}')\n",
        "#    with open('recording.webm', 'wb') as f:\n",
        "#      f.write(recorder_object)\n",
        "#    !ffmpeg -i recording.webm -ac 1 -f wav learner_file -y -hide_banner -loglevel panic\n",
        "    y_l,sr_l = librosa.load(learner_file)\n",
        "  elif mode == 'offline':\n",
        "    print(f'Prompt is: {prompts[idx-1]}')\n",
        "    y_l,sr_l = librosa.load(f'{learnerfiledir}f{idx}.wav')\n",
        "  else:\n",
        "    print('unknown mode')\n",
        "\n",
        "  # make temporary directory for alignments\n",
        "  !rm -rf /content/mfa/\n",
        "  !mkdir -p /content/mfa/sourcefiles/\n",
        "  !mkdir -p /content/mfa/aligned/\n",
        "\n",
        "  # write prompt text file to learner file dir\n",
        "#  with open (f'{tempalignmentdir}t{idx}.txt','w') as f:\n",
        "  with open (f'{tempalignmentdir}sourcefiles/f{idx}.txt','w') as f:\n",
        "#  with open (f'/content/mfa/aligned/t{idx}.txt','w') as f:\n",
        "    f.write(prompts[idx-1])\n",
        "\n",
        "  # load reference audio files\n",
        "  y_r,sr_r = librosa.load(ref_file)\n",
        "  # trim silence and mouse clicks from learner audio\n",
        "  y_trimmed = trim_silence(trim_clicks(y_l))\n",
        "  # write temporary copy of learner wave file to alignment directory\n",
        "  sf.write(f\"{tempalignmentdir}sourcefiles/f{idx}.wav\", y_trimmed, sr_l)\n",
        "\n",
        "  # get alignments\n",
        "  # this is SUPER slow, even with minimal lexicon\n",
        "  !source {INSTALL_DIR}/miniconda3/bin/activate aligner; mfa align --clean -j 1 /content/mfa/sourcefiles promptlex.txt english.zip /content/mfa/aligned\n",
        "  tg = textgrid.openTextgrid(f'/content/mfa/aligned/f{idx}.TextGrid',includeEmptyIntervals=False)\n",
        "#  textgrid_entries = tg.getTier('words')\n",
        "  textgrid_entries = tg.getTier('phones')\n",
        "  (spk_rate,vowel_len) = get_duration_statistics(textgrid_entries)\n",
        "  #print(f'speaking rate is {spk_rate:.2f} and average vowel duration is {vowel_len:.2f}')\n",
        "  print(f'phone sequence is {[p for (s,e,p) in textgrid_entries]}')\n",
        "  # PEPLOs pitch extraction\n",
        "  # average male: 120: average female: 220\n",
        "#  fo, voiced_flag, voiced_probs = librosa.pyin(y_l, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')) # original recording\n",
        "  f0, voiced_flag, voiced_probs = librosa.pyin(y_trimmed, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')) # trimmed recording\n",
        "  f0_ref, vf_ref, vp_ref = librosa.pyin(y_r, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')) # reference recording\n",
        "\n",
        "  # plot against length of longest utterance\n",
        "#  timeo = librosa.frames_to_time(np.arange(len(fo)), sr=sr_l) # original\n",
        "  time1 = librosa.frames_to_time(np.arange(len(f0)), sr=sr_l) # trimmed\n",
        "  time2 = librosa.frames_to_time(np.arange(len(f0_ref)), sr=sr_r) # reference\n",
        "  dur1 = librosa.get_duration(y=y_trimmed,sr=sr_l)\n",
        "  dur2 = librosa.get_duration(y=y_r,sr=sr_r)\n",
        "  dur_max = max(dur1,dur2)\n",
        "\n",
        "  # Plot waveform of learner - untrimmed\n",
        "#  plt.figure(figsize=(10, 3))\n",
        "#  librosa.display.waveshow(y_l, sr=sr_l, alpha=0.5)\n",
        "#  plt.xlim(0,dur_max)\n",
        "#  plt.xlabel(f'Time (s) - ORIGINAL UNTRIMMED')\n",
        "#  plt.ylabel('Amplitude')\n",
        "#  plt.show()\n",
        "\n",
        "  # Plot waveform of learner - trimmed\n",
        "#  plt.figure(figsize=(10, 3))\n",
        "#  librosa.display.waveshow(y_trimmed, sr=sr_l, alpha=0.5)\n",
        "#  plt.xlim(0,dur_max)\n",
        "#  plt.xlabel(f'Time (s) - TRIMMED')\n",
        "#  plt.ylabel('Amplitude')\n",
        "#  plt.show()\n",
        "\n",
        "  # Plot waveform of learner\n",
        "  plt.figure(figsize=(10, 3))\n",
        "  librosa.display.waveshow(y_trimmed, sr=sr_l, alpha=0.5)\n",
        "\n",
        "  # add alignments\n",
        "  for start, end, word in textgrid_entries:\n",
        "    plt.axvspan(start, end, color='lightgray', alpha=0.5)  # Highlight the word region\n",
        "    plt.text((start + end) / 2, -0.1, word, horizontalalignment='center', verticalalignment='center', fontsize=8, color='black')\n",
        "    plt.axvline(start,color='r',linestyle='--',alpha=0.15)\n",
        "    plt.axvline(end,color='r',linestyle='--',alpha=0.15)\n",
        "\n",
        "\n",
        "  plt.xlim(0,dur_max)\n",
        "  plt.xlabel(f'Time (s) - Learner speaking rate {spk_rate:.2f} phonemes/second and average vowel length {vowel_len:.2f} seconds')\n",
        "  plt.ylabel('Amplitude')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot contours\n",
        "  plt.figure(figsize=(10, 3))\n",
        "#  plt.plot(timeo, fo, color='r', label='untrimmed learner audio')\n",
        "  plt.plot(time1, f0, color='b', label='learner audio')\n",
        "  plt.plot(time2, f0_ref, color='g', label='reference audio')\n",
        "  plt.xlabel('Time (s)')\n",
        "  plt.ylabel('Frequency (Hz)')\n",
        "  plt.title('Pitch Contour')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # get alignments\n",
        "\n",
        "  # provide listening\n",
        "  print(\"Click the button below to play the learner audio:\")\n",
        "  display(Audio(y_trimmed, rate=sr_l))\n",
        "  print(\"Click the button below to play the reference audio:\")\n",
        "  display(Audio(y_r, rate=sr_r))\n",
        "\n",
        "def trim_clicks(audiofile):\n",
        "  # assume hop_length = 512\n",
        "  # click is about 0.05s\n",
        "  trim_dur = 512*3\n",
        "  return(audiofile[trim_dur:-trim_dur])\n",
        "\n",
        "def trim_silence(audiofile):\n",
        "  trimmed,index = librosa.effects.trim(audiofile,top_db=20)\n",
        "  # add a small amount of silence back for naturalness\n",
        "  buffer_length = 10*512 # 5 frames * 512 hop length\n",
        "  start_index = max(0,index[0] - buffer_length)\n",
        "  end_index = min(index[1] + buffer_length, len(audiofile))\n",
        "  audio_with_buffer = audiofile[start_index:end_index]\n",
        "  return(audio_with_buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfrFMngkfymH"
      },
      "outputs": [],
      "source": [
        "mode='live'\n",
        "if mode=='live':\n",
        "  camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "  recorder1 = AudioRecorder(stream=camera)\n",
        "  recorder2 = AudioRecorder(stream=camera)\n",
        "  recorder3 = AudioRecorder(stream=camera)\n",
        "  recorder4 = AudioRecorder(stream=camera)\n",
        "  recorder5 = AudioRecorder(stream=camera)\n",
        "  recorder6 = AudioRecorder(stream=camera)\n",
        "  recorder7 = AudioRecorder(stream=camera)\n",
        "  recorder8 = AudioRecorder(stream=camera)\n",
        "  recorder9 = AudioRecorder(stream=camera)\n",
        "  recorder10 = AudioRecorder(stream=camera)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51LqFxOzzd_O"
      },
      "source": [
        "##OFFLINE TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8ZR_B0exnPN"
      },
      "outputs": [],
      "source": [
        "plot_contours(3,mode='offline')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exBsY1zNzaIe"
      },
      "source": [
        "## LIVE TESTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh8seEfjHK2r"
      },
      "source": [
        "###Prompt 1\n",
        "\n",
        "**BOTH** code blocks have to be clicked in order to process a new recording.  The first one re-sets the recorder and asks you to press to record and press to stop.  Once you have recorded, click the second code block.  This will pick up your new recording and process it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4i7gQIILSVb"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[0]}\"')\n",
        "recorder1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKi1_xQYHSa6"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder1.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f1.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(1,mode='live',recorder_object=recorder1.audio.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D66E6wAtHZyM"
      },
      "source": [
        "###Prompt 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4GKlBalN7zx"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[1]}\"')\n",
        "recorder2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiKC8eKZgvOI"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder2.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f2.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLLbcXi3hNn0"
      },
      "source": [
        "###Prompt 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RveNK3_zhErX"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[2]}\"')\n",
        "recorder3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrd7gJ3rhEfy"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder3.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f3.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHCBJqz7wFZA"
      },
      "source": [
        "###Prompt 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIhgUwjhwEVQ"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[3]}\"')\n",
        "recorder4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXHeT0DBwIiv"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder4.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f4.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-4jbTRSwJHx"
      },
      "source": [
        "###Prompt 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvrX47_PwNtd"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[4]}\"')\n",
        "recorder5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD8fpW_TwNh_"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder5.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f5.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVcYNC31wPCn"
      },
      "source": [
        "###Prompt 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAey48ZbwSOQ"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[5]}\"')\n",
        "recorder6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBEixkgGwSAF"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder6.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f6.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnN4c7nhwS5F"
      },
      "source": [
        "###Prompt 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHs_wb5zwUuo"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[6]}\"')\n",
        "recorder7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxtI3o2AwUkn"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder7.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f7.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gP5gQ3iwVOQ"
      },
      "source": [
        "###Prompt 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79C30EcnwW3E"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[7]}\"')\n",
        "recorder8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ZzKgaiwWvn"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder8.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f8.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jggm1enwYHL"
      },
      "source": [
        "###Prompt 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geuhJF3-wZrB"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[8]}\"')\n",
        "recorder9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1aVBt8zwZjd"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder9.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f9.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00bNYeZXwaOR"
      },
      "source": [
        "###Prompt 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPbQz_2nwcI5"
      },
      "outputs": [],
      "source": [
        "print(f'Say the sentence \"{prompts[9]}\"')\n",
        "recorder10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A83Xb5eYwcCp"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder10.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav f10.wav -y -hide_banner -loglevel panic\n",
        "plot_contours(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ZCbJ1Pz155"
      },
      "source": [
        "##HANDY-MAYBE?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72gH4TeTz8fX"
      },
      "outputs": [],
      "source": [
        "# reading and writing wave files\n",
        "for i in range(1,len(prompts)+1):\n",
        "  y,sr = librosa.load(f'{learnerfiledir}f{i}.wav')\n",
        "  yt = trim_silence(trim_clicks(y))\n",
        "  sf.write(f\"ft{i}.wav\", yt, sr)\n",
        "\n",
        "#data, samplerate = sf.read('existing_file.wav')\n",
        "#sf.write('new_file.flac', data, samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA6IgliU2Of3"
      },
      "outputs": [],
      "source": [
        "# accessing text grid\n",
        "tg = textgrid.openTextgrid(f'/content/mfa/aligned/f3.TextGrid',includeEmptyIntervals=False)\n",
        "w =   tg.getTier('phones')\n",
        "vowel_count = 0\n",
        "vowel_duration = 0\n",
        "for (start,stop,word) in w.entries:\n",
        "  print(f'{start} {word} {stop}')\n",
        "  if any(digit in word for digit in ['0','1','2']):\n",
        "    vowel_count += 1\n",
        "    vowel_duration += (stop-start)\n",
        "\n",
        "\n",
        "nwords = len(w)\n",
        "utt_start = w.entries[0][0]\n",
        "utt_end = w.entries[-1][1]\n",
        "speaking_duration = utt_end-utt_start\n",
        "ave_vowel_dur = vowel_duration / vowel_count\n",
        "\n",
        "print (f'{nwords} phonemes starting at {utt_start} and ending at {utt_end} total time {speaking_duration}')\n",
        "print(f'speaking rate is {nwords/speaking_duration:.3f} phones per second')\n",
        "print(f'average vowel duration is {ave_vowel_dur:.3f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n__z-Cl0huY1"
      },
      "source": [
        "### EXTRA STUFF - IGNORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZZB6thrBec8"
      },
      "outputs": [],
      "source": [
        "## using torcoaudio to load and display audio\n",
        "import torchaudio\n",
        "yy,ss = torchaudio.load('/content/f10.wav')\n",
        "audiopath = '/content/f1.wav'\n",
        "ex_waveform, SR = torchaudio.load(audiopath)\n",
        "plt.plot(np.arange(0,ex_waveform.shape[1])/SR, ex_waveform[0])\n",
        "Audio(audiopath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUo147R9OVHY"
      },
      "outputs": [],
      "source": [
        "reffiledir = \"/content/reference_audio/\"\n",
        "# Load the two audio files (or one from user recording and another pre-recorded)\n",
        "y1, sr1 = librosa.load('f1.wav')   # Corresponding to the red pitch contour\n",
        "y2, sr2 = librosa.load(f'{reffiledir}r1.wav')  # Corresponding to the blue pitch contour\n",
        "\n",
        "# Extract pitch contours for both audio files using librosa's piptrack function\n",
        "pitches1, magnitudes1 = librosa.core.piptrack(y=y1, sr=sr1)\n",
        "pitches2, magnitudes2 = librosa.core.piptrack(y=y2, sr=sr2)\n",
        "\n",
        "# Extract the highest pitch for each frame where there's significant energy for the first audio\n",
        "pitch_contour1 = []\n",
        "for t in range(pitches1.shape[1]):\n",
        "    index = magnitudes1[:, t].argmax()\n",
        "    pitch = pitches1[index, t]\n",
        "    if pitch > 0:\n",
        "        pitch_contour1.append(pitch)\n",
        "    else:\n",
        "        pitch_contour1.append(np.nan)\n",
        "\n",
        "# Extract the highest pitch for each frame where there's significant energy for the second audio\n",
        "pitch_contour2 = []\n",
        "for t in range(pitches2.shape[1]):\n",
        "    index = magnitudes2[:, t].argmax()\n",
        "    pitch = pitches2[index, t]\n",
        "    if pitch > 0:\n",
        "        pitch_contour2.append(pitch)\n",
        "    else:\n",
        "        pitch_contour2.append(np.nan)\n",
        "\n",
        "# Plot the two pitch contours on the same plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(pitch_contour1, label='First Audio (Red)', color='red')\n",
        "plt.plot(pitch_contour2, label='Second Audio (Blue)', color='blue')\n",
        "plt.xlabel('Frames')\n",
        "plt.ylabel('Pitch (Hz)')\n",
        "plt.title('Pitch Contour of Two Audio Files')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Display two buttons to play each audio file\n",
        "print(\"Click the buttons below to play the corresponding audio files:\")\n",
        "\n",
        "# Audio player for the first audio (Red line)\n",
        "display(Audio(y1, rate=sr1))\n",
        "\n",
        "# Audio player for the second audio (Blue line)\n",
        "display(Audio(y2, rate=sr2))\n",
        "\n",
        "# Add side-by-side layout for the buttons\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"display:flex; justify-content: space-around;\">\n",
        "    <div>\n",
        "        <button onclick=\"document.querySelector('audio:nth-of-type(1)').play()\">Play Red Audio</button>\n",
        "    </div>\n",
        "    <div>\n",
        "        <button onclick=\"document.querySelector('audio:nth-of-type(2)').play()\">Play Blue Audio</button>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rpQgW0II6sm"
      },
      "outputs": [],
      "source": [
        "#import librosa.display\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "librosa.display.waveshow(y, sr=sr) # use waveplot should waveshow be unavailable\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvuvrNN4EWt8"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Upload an audio file\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# Assuming the audio file is uploaded and the filename is extracted\n",
        "#filename = list(uploaded.keys())[0]\n",
        "\n",
        "# 2. Load the audio file\n",
        "y, sr = librosa.load(\"file2.wav\", sr=None)  # y is the audio time series, sr is the sampling rate\n",
        "\n",
        "# 3. Extract pitch (fundamental frequency) using librosa's piptrack function\n",
        "# The piptrack function returns both the pitch and the magnitude for each frame.\n",
        "pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "\n",
        "# Extract the highest pitch for each frame where there's significant energy\n",
        "pitch_contour = []\n",
        "for t in range(pitches.shape[1]):\n",
        "    index = magnitudes[:, t].argmax()\n",
        "    pitch = pitches[index, t]\n",
        "    if pitch > 0:  # Filter out frames without detected pitch\n",
        "        pitch_contour.append(pitch)\n",
        "    else:\n",
        "        pitch_contour.append(np.nan)\n",
        "\n",
        "# 4. Plot the pitch contour\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(pitch_contour, label='Pitch Contour', color='blue')\n",
        "plt.xlabel('Frames')\n",
        "plt.ylabel('Pitch (Hz)')\n",
        "plt.title('Pitch Contour of the Audio File')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzvwa9RPHb8Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}